%!TEX TS-program = xelatex
%!TEX options = -aux-directory=Debug -shell-escape -file-line-error -interaction=nonstopmode -halt-on-error -synctex=1 "%DOC%"
\documentclass{article}
\input{LaTeX-Submodule/template.tex}

% Additional packages & macros
\usepackage{mathdots}
\setitemize{leftmargin=*,topsep=1ex,partopsep=0ex,itemsep=-1ex,partopsep=0ex,parsep=1ex}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\nullity}{nullity}

\usepackage{changepage} % Modify page width
\usepackage{multicol} % Use multiple columns
\usepackage[explicit]{titlesec} % Modify section heading styles

\titleformat{\section}{\raggedright\normalfont\bfseries}{}{0em}{#1}
\titleformat{\subsection}{\raggedright\normalfont\small\bfseries}{}{0em}{#1}

%% A4 page
\geometry{
a4paper,
margin = 10mm
}

%% Hide horizontal rule
\renewcommand{\headrulewidth}{0pt}
\fancyhead{}

%% Hide page numbers
\pagenumbering{gobble}

%% Multi-columns setup
\setlength\columnsep{4pt}

%% Paragraph setup
\setlength\parindent{0pt}
\setlength\parskip{0pt}

%% Customise section heading styles
% \titleformat*\section{\raggedright\bfseries}

\begin{document}
% Modify spacing
\titlespacing*\section{0pt}{0.5ex}{1ex}
\titlespacing*\subsection{0pt}{0.5ex}{1ex}
%
\setlength\abovecaptionskip{8pt}
\setlength\belowcaptionskip{-15pt}
\setlength\textfloatsep{0pt}
%
\setlength\abovedisplayskip{1pt}
\setlength\belowdisplayskip{1pt}

\begin{multicols*}{3}
    \subsection{General Solution to a Linear System}
    If \(\symbf{b} \in \columnspace{A}\): \(\symbf{x}_g = \symbf{x}_p +
    \symbf{x}_{ng}\) where \(\symbf{x}_p = \symbf{x}_r + \symbf{x}_n
    \in \R^n\) and \(\symbf{x}_{ng} =
    \vspan{\left(\nullspace{A}\right)}\).
    \subsection{Minimum Norm Solution}
    \(\symbf{x}_r \in \rowspace{A}\) where \(\symbf{x}_r = \proj_{\rowspace{A}}{\left( \symbf{x}_g \right)}\).
    \section{Least Squares (LS)}
    If \(\symbf{b} \not\in \columnspace{A}\): \(\displaystyle\symbf{x}
    = \argmin_{\symbf{x}^\ast \in \R^n} \norm{\symbf{b} -
    \symbf{A}\symbf{x}^\ast}\). \(\symbf{b} - \symbf{A} \symbf{x} \in
    \nullspace{A} \implies \symbf{A}^\top \left( \symbf{b} -
    \symbf{A}\symbf{x} \right) = \symbfup{0}\).
    % \begin{align*}
    %     \symbf{A}^\top \symbf{A}\symbf{x} & = \symbf{A}^\top \symbf{b} &  & \text{(Normal Equations)}
    % \end{align*}
    \subsection{Orthogonal Projection}
    \begin{align*}
        \symbf{P}           & = \symbf{A} \left( \symbf{A}^\top \symbf{A} \right)^{-1} \symbf{A}^\top \\
        \symbf{P} \symbf{b} & = \proj_{\columnspace{A}} \left( \symbf{b} \right) = \symbf{A}\symbf{x}
    \end{align*}
    \(\symbf{P}\) is idempotent (\(\symbf{P}^2 = \symbf{P}\)) and \(\symbf{P}^\top = \symbf{P}\).
    \subsection{Dependent Columns}
    If \(\nullity{\left( \symbf{A} \right)} > 0\), NE yields infinitely
    many solutions as \(\nullspace{A} = \mathcal{N}\left(\symbf{A}^\top
    \symbf{A}\right)\).
    \subsection{Orthogonal Complement Projections}
    Given \(\symbf{P} = \proj_V\): \(\symbf{Q} = \proj_{V^\perp} =
    \symbf{I} - \symbf{P}\)
    \begin{equation*}
        \symbf{b} = \proj_V(\symbf{b}) + \proj_{V^\perp}(\symbf{b}) = \symbf{P}\symbf{b} + \symbf{Q}\symbf{b}
    \end{equation*}
    \begin{align*}
        \left( \symbf{P}\symbf{b} \right)^\top \symbf{Q}\symbf{b} & = 0                                   \\
        \symbf{P} \symbf{Q}                                       & = \symbf{0} &  & \text{(zero matrix)}
    \end{align*}
    \subsection{Change of Basis}
    Given the basis \(W = \left\{ \symbf{w}_1,\: \dots,\: \symbf{w}_n
    \right\}\)
    \begin{align*}
        \symbf{b} & = c_1 \symbf{w}_1 + \cdots + c_n \symbf{w}_n                       \\
        \symbf{b} & = \symbf{W} \symbf{c} \iff \left( \symbf{b} \right)_W = \symbf{c}.
    \end{align*}
    \subsection{Orthonormal Basis}
    Normalised and orthogonal basis vectors. For \(Q = \left\{
    \symbf{q}_1,\: \dots,\: \symbf{q}_n \right\}\), \(\symbf{q}_i^\top
    \symbf{q}_j = \delta_{ij}\), where
    \begin{equation*}
        \delta_{ij} =
        \begin{cases}
            1, & i = j   \\
            0, & i \ne j
        \end{cases}
    \end{equation*}
    \begin{equation*}
        \symbf{Q} \symbf{c} = \symbf{b}
        \iff
        \symbf{Q}^\top \symbf{b} = \symbf{c} = \left( \symbf{b} \right)_Q
    \end{equation*}
    \subsection{Orthogonal Matrices}
    \begin{equation*}
        \symbf{Q}^\top = \symbf{Q}^{-1}
        \iff
        \symbf{Q}^\top \symbf{Q} = \symbf{Q}\symbf{Q}^\top = \symbf{I}.
    \end{equation*}
    \subsection{Projection onto a Vector}
    \begin{align*}
        \proj_{\symbf{a}} \left( \symbf{b} \right) & = \symbf{a} \left( \symbf{a}^\top \symbf{a} \right)^{-1} \symbf{a}^\top \symbf{b} \\
                                                   & = \frac{\symbf{a}}{\norm{\symbf{a}}^2} \symbf{a} \cdot \symbf{b}
    \end{align*}
    \begin{align*}
        \proj_{\symbf{q}} \left( \symbf{b} \right) = \symbf{q} \left( \symbf{q} \cdot \symbf{b} \right) && \text{(unit vector)} \\
    \end{align*}
    \subsection{Gram-Schmidt Process}
    Converts the basis \(W\) that spans \(\columnspace{A}\) to an
    orthonormal basis \(Q\).
    \begin{align*}
        \symbf{v}_i & = \symbf{w}_i - \sum_{j = 1}^{i - 1} \symbf{q}_j \abracket*{\symbf{q}_j,\: \symbf{w}_i} & \symbf{q}_i & = {\scriptstyle \symbf{v}_i / \norm*{\symbf{v}_i}}
    \end{align*}
    \(V\) and \(Q\) span \(W\), and \(V\) is orthogonal.
    \subsection{QR Decomposition}
    \begin{align*}
        \symbf{A} & = \symbf{Q} \symbf{R}                                                                                                                                                                  \\
        \symbf{R} & =
                      \begin{bmatrix}
                          \norm{\symbf{v}_1} & \abracket*{\symbf{q}_1,\: \symbf{w}_2} & \cdots & \abracket*{\symbf{q}_1,\: \symbf{w}_n}       \\
                          0                  & \norm{\symbf{v}_2}                     & \ddots & \vdots                                       \\
                          \vdots             & \ddots                                 & \ddots & \abracket*{\symbf{q}_{n - 1},\: \symbf{w}_n} \\
                          0                  & \cdots                                 & 0      & \norm{\symbf{v}_n}
                      \end{bmatrix}
    \end{align*}
    where \(\symbf{Q}\) is found by applying the Gram-Schmidt process and \(\symbf{R}\) is upper triangular. \(\symbf{R} \symbf{x} = \symbf{Q}^\top \symbf{b}\) solves LS\@.

    \section{Eigenvalues and Eigenvectors}
    \begin{equation*}
        \symbf{A} \symbf{v} = \lambda \symbf{v} \iff \left( \lambda \symbf{I} - \symbf{A} \right) \symbf{v} = \symbfup{0} : \symbf{v} \neq \symbfup{0}
    \end{equation*}
    \subsection{Characteristic Polynomial}
    \begin{equation*}
        P\left( \lambda \right) = \det{\left( \lambda \symbf{I} - \symbf{A} \right)} = 0.
    \end{equation*}
    \subsection{Eigen Decomposition}
    \begin{align*}
        \symbf{A} \symbf{V} = \symbf{V} \symbf{D}
                  & \iff
        \symbf{A} = \symbf{V} \symbf{D} \symbf{V}^{-1}                       \\
        \symbf{V} & =
                      \begin{bmatrix}
                          \symbf{v}_1 & \cdots & \symbf{v}_n
                      \end{bmatrix}
        \\
        \symbf{D} & = \diag{\left( \lambda_1,\: \dots,\: \lambda_n \right)}.
    \end{align*}
    \subsection{Eigenspace}
    The eigenspace associated with \(\lambda_i\) is the span of
    eigenvectors: \(\mathcal{N}{\left( \lambda_i \symbf{I} - \symbf{A}
    \right)}\).
    \subsection{Algebraic Multiplicity \texorpdfstring{\(\mu\left( \lambda_i \right)\)}{mu(lambda i)}}
    Multiplicity of \(\lambda_i\) in \(P(\lambda)\),
    for \(d \leq n\) distinct eigenvalues,
    \begin{equation*}
        P\left( \lambda \right) = \left( \lambda - \lambda_1 \right)^{\mu\left( \lambda_1 \right)} \cdots \left( \lambda - \lambda_d \right)^{\mu\left( \lambda_d \right)}.
    \end{equation*}
    In general
    \begin{gather*}
        1 \leq \mu\left( \lambda_i \right) \leq n         \\
        \sum_{i = 1}^d \mu \left( \lambda_i \right) = n
    \end{gather*}
    If \(\nullity{\left( \symbf{A} \right)} > 0\)
    \begin{equation*}
        \exists k : \lambda_k = 0 : \mu\left( \lambda_k \right) = \nullity{\left( \symbf{A} \right)}
    \end{equation*}
    \subsection{Geometric Multiplicity \texorpdfstring{\(\gamma\left( \lambda_i \right)\)}{gamma(lambda i)}}
    The dimension of each eigenspace \(\lambda_i\)
    \begin{equation*}
        \gamma \left( \lambda_i \right) = \nullity{\left( \lambda_i \symbf{I} - \symbf{A} \right)}.
    \end{equation*}
    Given \(d \leq n\) distinct eigenvalues,
    \begin{gather*}
        1 \leq \gamma\left( \lambda_i \right) \leq \mu\left( \lambda_i \right) \leq n \\
        d \leq \sum_{i = 1}^d \gamma \left( \lambda_i \right) \leq n.
    \end{gather*}
    Eigenvectors corresponding to distinct eigenvalues are linearly dependent.
    \subsection{Defective Matrix}
    \(\symbf{A}\) lacks a complete eigenbasis:
    \begin{equation*}
        \exists k : \gamma\left( \lambda_k \right) < \mu\left( \lambda_k \right)
    \end{equation*}
    \subsection{Matrix Similarity}
    \(\symbf{A}\) and \(\symbf{B}\) are similar if \(\symbf{B} = \symbf{P}^{-1} \symbf{A} \symbf{P}\).
    They share \(P(\lambda)\), ranks, determinants, traces, and
    eigenvalues (also \(\mu\) and \(\gamma\)).
    \subsection{Symmetric Matrices \texorpdfstring{\(\symbf{S}^\top = \symbf{S}\)}{S' = S}}
    \(\symbf{S}\) is always diagonalisable and has
    real eigenvalues with real orthogonal eigenspaces: \(\symbf{S} = \symbf{Q} \symbf{D} \symbf{Q}^\top\), where \(\symbf{Q}\) is found through QR\@: \(\symbf{V} = \symbf{Q} \symbf{R}\).
    \subsection{Skew-Symmetric Matrices \texorpdfstring{\(\symbf{K}^\top = -\symbf{K}\)}{K' = -K}}
    Eigenvalues are always purely imaginary.
    \subsection{Positive-Definite Matrices}
    \(\symbf{S}\) is (symmetric) positive definite (SPD) if all its eigenvalues are positive, likewise
    \begin{equation*}
        \symbf{x}^\top \symbf{S} \symbf{x} > 0 : \forall \symbf{x} \in \R^n \backslash \left\{ \symbfup{0} \right\}
    \end{equation*}
    \subsection{Matrix Functions}
    Given a nondefective matrix:
    \begin{align*}
        f\left( \symbf{A} \right) & = \symbf{V} f\left( \symbf{D} \right) \symbf{V}^{-1}                                                               \\
                                  & = \symbf{V} \diag{\left( f\left( \lambda_1 \right),\: \ldots,\: f\left( \lambda_n \right) \right)} \symbf{V}^{-1}
    \end{align*}
    for an analytic function \(f\).
    \subsection{Cayley-Hamilton Theorem}
    \begin{align*}
        \forall \symbf{A} : P\left( \symbf{A} \right) = \symbfup{0} &  & \text{(zero matrix)}
    \end{align*}
    \section{Singular Value Decomposition}
    \begin{gather*}
        \symbf{A} \symbf{V} = \symbf{U} \symbf{\Sigma}
        \iff
        \symbf{A} = \symbf{U} \symbf{\Sigma} \symbf{V}^\top                                        \\
        \symbf{V}^\top = \symbf{V}^{-1}, \quad \symbf{U}^\top = \symbf{U}^{-1}                                                          \\
        \symbf{\Sigma} = \diag{\left( \sigma_1,\: \dots,\: \sigma_r,\: 0,\: \dots,\: 0 \right)}.
    \end{gather*}
    Left singular vectors \(\symbf{u}\): \(\symbf{U} \in \R^{m \times m}\)
    \begin{align*}
        \columnspace{A}   & = \vspan{\left( \left\{ \symbf{u}_{i \leq r} \right\} \right)}     \\
        \leftnullspace{A} & = \vspan{\left( \left\{ \symbf{u}_{r < i \leq m} \right\} \right)}
    \end{align*}
    Right singular vectors \(\symbf{v}\): \(\symbf{V} \in  \R^{n \times n}\)
    \begin{align*}
        \rowspace{A}  & = \vspan{\left( \left\{ \symbf{v}_{i \leq r} \right\} \right)}     \\
        \nullspace{A} & = \vspan{\left( \left\{ \symbf{v}_{r < i \leq n} \right\} \right)}
    \end{align*}
    Singular values \(\sigma_i\): \(\symbf{\Sigma} \in \R^{m \times n}\)

    The eigenvalues of \(\symbf{A}^\top\symbf{A}\) and
    \(\symbf{A}\symbf{A}^\top\) are equal, \(\symbf{\Sigma}^\top
    \symbf{\Sigma}\) and \(\symbf{\Sigma} \symbf{\Sigma}^\top\) have
    the same diagonal entries, and when \(m = n\),
    \(\symbf{\Sigma}^\top\symbf{\Sigma} = \symbf{\Sigma}
    \symbf{\Sigma}^\top = \symbf{\Sigma}^2\). To find \(\sigma_i\)
    compute:
    \begin{align*}
        \symbf{A}^\top \symbf{A} & = \symbf{V} \symbf{\Sigma}^\top \symbf{\Sigma} \symbf{V}^\top \\
        \symbf{A} \symbf{A}^\top & = \symbf{U} \symbf{\Sigma} \symbf{\Sigma}^\top \symbf{U}^\top
    \end{align*}
    so that \(\sigma_i = \sqrt{\lambda}_i\) where \(\sigma_1 \geq \cdots \geq \sigma_r > 0\).
    \subsection{Reduced SVD}
    Ignores \(m - n\) ``0'' rows in \(\symbf{\Sigma}\) so that
    \(\symbf{U} \in \R^{m \times n}\), \(\symbf{\Sigma} \in \R^{n
    \times n}\), and \(\symbf{V} \in \R^{n \times n}\).
    \subsection{Pseudoinverse}
    Consider the inverse mapping \(\symbf{u}_i \mapsto
    \frac{1}{\sigma_i} \symbf{v}_i\)
    \begin{gather*}
        \symbf{A}^\dagger \symbf{u}_i = \frac{1}{\sigma_i} \symbf{v}_i
        \iff
        \symbf{A}^\dagger \symbf{u}_i = \frac{1}{\sigma_i} \symbf{v}_i \symbf{u}^\top_i \symbf{u}_i \\
        \symbf{A}^\dagger = \sum_{i = 1}^r \frac{1}{\sigma_i} \symbf{v}_i \symbf{u}^\top_i
        \iff
        \symbf{A}^\dagger = \symbf{V} \symbf{\Sigma}^\dagger \symbf{U}^\top
    \end{gather*}
    where \(\symbf{\Sigma}^\dagger = \diag{\left( \frac{1}{\sigma_1},\: \dots,\: \frac{1}{\sigma_r},\: 0,\: \dots,\: 0 \right)}\).
    \(\symbf{x} = \symbf{A}^\dagger \symbf{b}\) also solves LS\@.
    \subsection{Truncated SVD}
    Express \(\symbf{A}\) as the sum of rank-1 matrices:
    \begin{equation*}
        \symbf{A} = \sum_{i = 1}^n \sigma_i \symbf{u}_i \symbf{v}^\top_i \approx \tilde{\symbf{A}} = \sum_{i = 1}^\nu \sigma_i \symbf{u}_i \symbf{v}^\top_i
    \end{equation*}
    for the rank-\(\nu\) approximation of \(\symbf{A}\).
    Using the SVD\@:
    \begin{equation*}
        \tilde{\symbf{A}} = \symbf{U} \symbf{\Sigma} \symbf{V}^\top
    \end{equation*}
    \(\symbf{U} \in \R^{m \times \nu}\), \(\symbf{\Sigma} \in \R^{\nu \times \nu}\), and \(\symbf{V} \in \R^{n \times \nu}\).
    When \(\nu \geq r\), \(\tilde{\symbf{A}} = \symbf{A}\) as
    \(\sigma_{i > r} = 0\).
    \section{General Vector Spaces}
    \(V\) is a vector space with vectors \(\symbf{v} \in V\) if the following 10 axioms are satisfied
    for \(\forall \symbf{u}, \symbf{v}, \symbf{w} \in V\) and \(\forall k, m \in \mathbb{F}\),
    given an addition and scalar multiplication operation.

    \underline{For the addition operation:}
    \begin{itemize}
        \item Closure: \(\symbf{u} + \symbf{v} \in V\)
        \item Commutativity: \(\symbf{u} + \symbf{v} = \symbf{v} +
              \symbf{u} \in V\)
        \item Associativity:
              \begin{equation*}
                  \symbf{u} + \left( \symbf{v} + \symbf{w} \right) = \left( \symbf{u} + \symbf{v} \right) + \symbf{w}
              \end{equation*}
        \item Identity: \(\exists \symbfup{0} \in V : \symbf{u} +
              \symbfup{0} = \symbfup{0} + \symbf{u} = \symbf{u}\)
        \item Inverse: \(\exists \left( -\symbf{u} \right) \in V :
              \symbf{u} + \left( -\symbf{u} \right) = \symbfup{0}\)
    \end{itemize}
    \underline{For the scalar multiplication operation:}
    \begin{itemize}
        \item Closure: \(k \symbf{u} \in V\)
        \item Distributivity: \(k \left( \symbf{u} + \symbf{v} \right)
              = k\symbf{u} + k\symbf{v}\)
        \item Distributivity: \(\left( k + m \right) \symbf{u} =
              k\symbf{u} + m\symbf{u}\)
        \item Associativity: \(k \left( m\symbf{u} \right) = \left( k m
              \right) \symbf{u}\)
        \item Identity: \(\exists 1 \in \mathbb{F} : 1 \symbf{u} =
              \symbf{u}\)
    \end{itemize}
    \subsection{Examples of Vector Spaces}
    The set of all \(m \times n\) matrices \(\mathscr{M}_{mn}\) with
    matrix addition and scalar matrix multiplication.

    The set of all functions \(\mathscr{F}\left( \Omega \right) :
    \Omega \to \R\) with addition and scalar multiplication defined
    pointwise.
    \subsection{Subspaces}
    The subset \(W \subset V\) is itself a vector space if it is closed
    under addition and scalar multiplication.
    \subsection{Examples of Subspaces}
    \underline{Subspaces of \(\R^n\):}
    \begin{itemize}
        \item Lines, planes and higher-dimensional analogues in
              \(\R^n\) \emph{passing through the origin}.
    \end{itemize}
    \underline{Subspaces of \(\mathscr{M}_{nn}\):}
    \begin{itemize}
        \item The set of all \emph{symmetric} \(n \times n\) matrices,
              denoted \(\mathscr{S}_n \subset \mathscr{M}_{nn}\).
        \item The set of all \emph{skew symmetric} \(n \times n\)
              matrices, denoted \(\mathscr{K}_n \subset
              \mathscr{M}_{nn}\).
    \end{itemize}
    \underline{Subspaces of \(\mathscr{F}\):}
    \begin{itemize}
        \item The set of all \emph{polynomials} of degree \(n\) or
              less, denoted \(\mathscr{P}_n\left( \Omega \right)
              \subset \mathscr{F}\left( \Omega \right)\).
        \item The set of all \emph{continuous functions}, denoted
              \(C\left( \Omega \right) \subset \mathscr{F}\left( \Omega
              \right)\).
        \item The set of all continuous functions with \emph{continuous
              \(n\)th derivatives}, denoted \(C^n\left( \Omega \right)
              \subset C\left( \Omega \right)\).
        \item The set of all functions \(f\) defined on
              \(\interval{0}{1}\) satisfying \(f\left( 0 \right) =
              f\left( 1 \right)\).
    \end{itemize}
    \subsection{General Vector Space Terminology}
    Let \(S = \left\{ \symbf{v}_1,\: \dots,\: \symbf{v}_k \right\}\)
    and \(c_1,\: \dots,\: c_k \in \mathbb{F}\):
    \begin{itemize}
        \item The linear combination of \(S\) is a vector of the form
              \(\symbf{v} = c_1 \symbf{v}_1 + \cdots + c_k
              \symbf{v}_k\).
        \item \(S\) is linearly independent iff
              \(c_1 \symbf{v}_1 + \cdots + c_k \symbf{v}_k = \symbfup{0}\) has the trivial solution.
        \item \(\vspan{\left( S \right)}\) is the set of all linear combinations of \(S\).
    \end{itemize}
    \(S\) is a \textit{basis} for a vector space \(V\) if
    \begin{itemize}
        \item \(S\) is linearly independent.
        \item \(\vspan{\left( S \right)} = V\).
    \end{itemize}
    The number of basis vectors denotes the dimension of \(V\).
    \(C\) is infinite dimensional.
    \subsection{Examples of Standard Bases}
    \begin{itemize}
        \item \(\mathscr{M}_{22}\):
              \begin{equation*}
                  \left\{
                  \begin{bmatrix*}
                      1 & 0 \\
                      0 & 0
                  \end{bmatrix*}
                  ,\:
                  \begin{bmatrix*}
                      0 & 0 \\
                      1 & 0
                  \end{bmatrix*}
                  ,\:
                  \begin{bmatrix*}
                      0 & 1 \\
                      0 & 0
                  \end{bmatrix*}
                  ,\:
                  \begin{bmatrix*}
                      0 & 0 \\
                      0 & 1
                  \end{bmatrix*}
                  \right\}
              \end{equation*}
        \item \(\mathscr{S}_{22}\): \(\left\{
              \begin{bmatrix*}
                  1 & 0 \\
                  0 & 0
              \end{bmatrix*}
              ,\:
              \begin{bmatrix*}
                  0 & 1 \\
                  1 & 0
              \end{bmatrix*}
              ,\:
              \begin{bmatrix*}
                  0 & 0 \\
                  0 & 1
              \end{bmatrix*}
              \right\}\)
        \item \(\mathscr{K}_{22}\): \(\left\{
              \begin{bmatrix*}
                  0 & 1 \\
                  -1 & 0
              \end{bmatrix*}
              \right\}\)
        \item \(\mathscr{P}_3\): \(\left\{ 1,\: x,\: x^2,\: x^3 \right\}\)
    \end{itemize}
    \subsection{Linear Transformations}
    \(T:V \to W\) satisfying
    \begin{align*}
        T\left( k\symbf{u} \right)            & = k T\left( \symbf{u} \right)                           \\
        T\left( \symbf{u} + \symbf{v} \right) & = T\left( \symbf{u} \right) + T\left( \symbf{v} \right)
    \end{align*}
    \underline{Constructing \(\symbf{A} = \left( T \right)_{B',\: B}\):}

    Consider the map of \(\left( \symbf{v} \right)_B = \symbf{x}\) of
    \(\symbf{v} \in V\) to \(\left( \symbf{w} \right)_{B'} =
    \symbf{b}\) of \(\symbf{w} \in W\), where \(B = \left\{
    \symbf{v}_1,\: \dots,\: \symbf{v}_n \right\}\) and \(B' = \left\{
    \symbf{w}_1,\: \dots,\: \symbf{w}_m \right\}\).
    \begin{align*}
        T\left( \symbf{v} \right) & = \symbf{w}           \\
        \begin{bmatrix}
            T\left( \symbf{v}_1 \right) & \cdots & T\left( \symbf{v}_n \right)
        \end{bmatrix}
        \symbf{x}                 & = \symbf{W} \symbf{b} \\
        \begin{bmatrix}
            \left( T\left( \symbf{v}_1 \right) \right)_{B'} & \cdots & \left( T\left( \symbf{v}_n \right) \right)_{B'}
        \end{bmatrix}
        \symbf{x}                 & = \symbf{b}           \\
        \symbf{A} \symbf{x}       & = \symbf{b}
    \end{align*}
    \subsection{Isomorphism (\texorpdfstring{\(\cong\)}{≅})}
    \(T : V \to W\) is an isomorphism between \(V\) and \(W\) if there exists a bijection between the two vector spaces.
    \(\forall V : \dim{\left( V \right)} = n : V \cong \R^n\), \(\mathscr{M}_{mn} \cong \R^{mn}\)
    and \(\mathscr{P}_n \cong \R^{n + 1}\).
    \subsection{Fundamental Subspaces of \texorpdfstring{\(T\)}{T}}
    \begin{itemize}
        \item The set of all vectors in \(V\) that map to \(W\) is the
              \textbf{image} of \(T\), denoted \(\vim{\left( T
              \right)}\).
        \item The set of all vectors in \(V\) that \(T\) maps to
              \(\symbfup{0}_W\) is the \textbf{kernel} of \(T\),
              denoted \(\vker{\left( T \right)}\).
    \end{itemize}
    If finite, \(\dim{\left( \vim{\left( T \right)} \right)} = \vrank{\left( T \right)}\)
    and \(\dim{\left( \vker{\left( T \right)} \right)} = \nullity{\left( T \right)}\).
    \begin{equation*}
        \vrank{\left( T \right)} + \nullity{\left( T \right)} = \dim{\left( V \right)}.
    \end{equation*}
    \subsection{Inner Product Spaces}
    \begin{equation*}
        \abracket*{\cdot,\: \cdot} : V \times V \to \R.
    \end{equation*}
    For \(\symbf{u},\: \symbf{v},\: \symbf{w} \in V\)
    and \(k \in \R\):
    \begin{itemize}
        \item Symmetry: \(\abracket*{\symbf{u},\: \symbf{v}} =
              \abracket*{\symbf{v},\: \symbf{u}}\)
        \item Linearity:
              \begin{equation*}
                  \abracket*{\symbf{u} + \symbf{v},\: \symbf{w}} = \abracket*{\symbf{u},\: \symbf{w}} + \abracket*{\symbf{v},\: \symbf{w}}
              \end{equation*}
        \item Linearity: \(\abracket*{k \symbf{u},\: \symbf{v}} =
              k\abracket*{\symbf{u},\: \symbf{v}}\)
        \item Positive semi-definitiveness:
              \begin{equation*}
                  \abracket*{\symbf{u},\: \symbf{u}} \geq 0,\: \abracket*{\symbf{u},\: \symbf{u}} = 0 \iff \symbf{u} = \symbfup{0}
              \end{equation*}
    \end{itemize}
    \underline{For \(\symbf{u},\: \symbf{v} \in \R^n\):}
    \begin{itemize}
        \item \(\abracket*{\symbf{u},\: \symbf{v}} = \symbf{u} \cdot \symbf{v} = \symbf{u}^\top \symbf{v}\).
        \item \(\abracket*{\symbf{u},\: \symbf{v}} = \symbf{u}^\top \symbf{A} \symbf{v}\) where \(\symbf{A}\) is SPD\@.
    \end{itemize}
    \underline{For \(\symbf{A},\: \symbf{B} \in \mathscr{M}_{mn}\):}
    \begin{itemize}
        \item \(\abracket*{\symbf{A},\: \symbf{B}} = \Tr{\left( \symbf{A}^\top \symbf{B} \right)}\).
    \end{itemize}
    \underline{For \(f,\: g \in C\left( \interval{a}{b} \right)\):}
    \begin{itemize}
        \item \(\abracket*{f,\: g} = \int_a^b f\left( x \right) g\left( x \right) \odif{x}\)
        \item \(\abracket*{f,\: g} = \int_a^b f\left( x \right) g\left( x \right) w\left( x \right) \odif{x}\)
    \end{itemize}
    where \(w\left( x \right) > 0 : \forall x \in \interval{a}{b}\).
    \subsection{Norms}
    \begin{itemize}
        \item \(\norm*{\symbf{v}} = \sqrt{\abracket*{\symbf{v},\: \symbf{v}}}\).
        \item \(\norm*{\symbf{v}} \geq 0\), and \(\norm*{\symbf{v}} = 0 \iff \symbf{v} = \symbfup{0}\).
        \item \(\norm*{k \symbf{v}} = \abs*{k} \norm*{\symbf{v}} : \forall k \in \R\).
        \item \(\norm*{\symbf{u} + \symbf{v}} \leq \norm*{\symbf{u}} + \norm*{\symbf{v}}\).
    \end{itemize}
    \underline{Examples:}
    \begin{itemize}
        \item \(\forall \symbf{A} \in \mathscr{M}_{mn} : \norm*{\symbf{A}} = \sqrt{\sum_{i = 1}^m \sum_{j = 1}^n a_{ij}^2}\).
        \item \(\forall f \in C\left( \interval{a}{b} \right) : \norm*{f} = \sqrt{\int_a^b f\left( x \right)^2 \odif{x}}\).
    \end{itemize}
    \subsection{Orthogonality}
    \begin{equation*}
        \abracket*{\symbf{v},\: \symbf{v}} = 0.
    \end{equation*}
    \subsection{Orthogonal Complements of \texorpdfstring{\(\mathscr{M}_{n}\)}{Mn}}
    Given \(\symbf{P}_{\mathscr{S}_n} = \proj_{\mathscr{S}_n}\) and
    \(\symbf{P}_{\mathscr{K}_n} = \proj_{\mathscr{K}_n}\)
    \begin{equation*}
        \symbf{P}_{\mathscr{S}_n} = \symbf{I} - \symbf{P}_{\mathscr{K}_n}                                                                                                    \\
    \end{equation*}
    \begin{align*}
        \symbf{S} & = \symbf{P}_{\mathscr{S}_n} \symbf{M} = \proj_{\symbf{P}_{\mathscr{S}_n}}{\left( \symbf{M} \right)} = \frac{\symbf{M} + \symbf{M}^\top}{2} \\
        \symbf{K} & = \symbf{P}_{\mathscr{K}_n} \symbf{M} = \proj_{\symbf{P}_{\mathscr{K}_n}}{\left( \symbf{M} \right)} = \frac{\symbf{M} - \symbf{M}^\top}{2}
    \end{align*}
    \(\symbf{S} \in \mathscr{S}_n\), \(\symbf{K} \in \mathscr{K}_n\), and \(\symbf{S} + \symbf{K} = \symbf{M} \in \mathscr{M}_n\).
    \section{Theorems}
    \begin{itemize}
        \item \(\symbf{A}^\top \symbf{A}\) is always positive semi-definite,
              and \(\mathcal{N}\left( \symbf{A}^\top\symbf{A} \right) = \nullspace{A}\) so that
              \(\vrank{\left( \symbf{A}^\top \symbf{A} \right) = \vrank{\left( \symbf{A} \right)}}\).
              \(\symbf{A}^\top \symbf{A}\) is positive definite and
              \(\symbf{A}^\top \symbf{A}\) is invertible when \(\nullity{\left( \symbf{A} \right)} = 0\).
        \item When \(\symbf{A}\) is square and invertible, \(\left(
              \symbf{A}^\top \symbf{A} \right)^{-1} = \symbf{A}^{-1}
              \symbf{A}^{-\top}\) and \(\symbf{P} = \symbf{I}\)
              otherwise \(\symbf{P} = \symbf{Q} \symbf{Q}^\top\) using
              QR\@.
        \item \(\symbf{P}^2 = \symbf{P} \land \symbf{P}^\top = \symbf{P} \iff \symbf{P} = \proj_{\columnspace{P}}\).
              \(\symbf{P} \symbf{v} = \symbf{P}^2 \symbf{v} \iff \lambda \symbf{v} = \lambda^2 \symbf{v}\) implies \(\lambda = 0,\: 1\).
        \item \(\symbf{A}^\top \symbf{A}\) and \(\symbf{A} \symbf{A}^\top\) share eigenvalues,
              \begin{align*}
                  \symbf{A}^\top \symbf{A} \symbf{v}                                         & = \lambda \symbf{v}                           \\
                  \left( \symbf{A} \symbf{A}^\top \right) \left( \symbf{A} \symbf{v} \right) & = \lambda \left( \symbf{A} \symbf{v} \right).
              \end{align*}
              \(\symbf{A} \symbf{v} = \symbfup{0} \implies \lambda = 0\), else \(\symbf{w} = \symbf{A} \symbf{v}\) is an eigenvector of \(\symbf{A} \symbf{A}^\top\).
        \item For symmetric \(\symbf{S} \in \R^{n \times n}\):
              \begin{equation*}
                  \symbf{S} = \sum_{i = 1}^n \lambda_i \symbf{q}_i \symbf{q}_i^\top = \sum_{i = 1}^n \lambda_i \proj_{\symbf{q}_i}
              \end{equation*}
        \item For \(\symbf{W} = \symbf{w} \in \R^{n \times 1}\):
              \begin{align*}
                  \symbf{W}         & =
                                        \begin{bmatrix*}
                                            \hat{\symbf{w}}
                                        \end{bmatrix*}
                  \begin{bmatrix*}
                      \norm*{\symbf{w}}
                  \end{bmatrix*}
                  \begin{bmatrix*}
                      1
                  \end{bmatrix*}
                  \\
                  \symbf{W}^\dagger & = \hat{\symbf{w}}^\top / \norm*{\symbf{w}}
              \end{align*}
    \end{itemize}
    \section{Identities}
    \begin{itemize}
        \item \(\left( \symbf{A} \symbf{B} \right)^\top = \symbf{B}^\top \symbf{A}^\top\).
        \item \(\left( \symbf{A} \symbf{B} \right)^{-1} = \symbf{B}^{-1} \symbf{A}^{-1}\) if \(\symbf{A}\), \(\symbf{B}\) invertible.
        \item \(\left( \symbf{A}^\top \right)^{-1} = \left( \symbf{A}^{-1} \right)^\top\) if \(\symbf{A}\) invertible:
              \begin{align*}
                  \symbf{A}^\top \left( \symbf{A}^{-1} \right)^\top & = \left( \symbf{A}^{-1} \symbf{A} \right)^\top = \symbf{I} \\
                  \left( \symbf{A}^{-1} \right)^\top \symbf{A}^\top & = \left( \symbf{A} \symbf{A}^{-1} \right)^\top = \symbf{I}
              \end{align*}
        \item \(\abracket*{\symbf{A}\symbf{x},\: \symbf{y}} = \abracket*{\symbf{x},\: \symbf{A}^\top \symbf{y}}\):
              \begin{equation*}
                  \left( \symbf{A} \symbf{x} \right)^\top \symbf{y} = \symbf{x}^\top \left( \symbf{A}^\top \symbf{y} \right)
              \end{equation*}
        \item \(\det{\left( \symbf{A} \symbf{B} \right)} = \det{\left( \symbf{A} \right)} \det{\left( \symbf{B} \right)}\).
        \item If \(\symbf{A}\) is triangular, \(\det{\left( \symbf{A}
              \right)} = \prod_{i = 1}^n a_{ii}\).
        \item For \(\symbf{A} \in \R^{n \times n}\):
              \begin{align*}
                  \Tr{\left( \symbf{A} \right)}                 & = \sum_{i = 1}^n a_{ii} = \sum_{i = 1}^n \lambda_i              \\
                  \det{\left( \symbf{A} \right)}                & = \prod_{i = 1}^n \lambda_i                                     \\
                  \det{\left( \symbf{A}^\top \symbf{A} \right)} & = \det{\left( \symbf{A} \right)}^2 = \prod_{i = 1}^n \sigma_i^2
              \end{align*}
        \item For \(\symbf{A} \in \R^{m \times n}\):
              \begin{align*}
                  \Tr{\left( \symbf{A}^\top \symbf{A} \right)}  & = \sum_{j = 1}^m \sum_{i = 1}^n a_{ij}^2 \\
                                                                & = \sum_{i = 1}^n \sigma_i^2              \\
                  \det{\left( \symbf{A}^\top \symbf{A} \right)} & = \prod_{i = 1}^n \sigma_i^2
              \end{align*}
    \end{itemize}
\end{multicols*}
\end{document}
